<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dremio | reimagining data analytics for the modern world</title>
    <link>kstirman.github.io/bookshelf/categories/cluster/index.xml</link>
    <description>Recent content on dremio | reimagining data analytics for the modern world</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="kstirman.github.io/bookshelf/categories/cluster/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Old and busted: Teasing formerly-fashionable websites from Reddit data</title>
      <link>/kstirman.github.io/bookshelf/blog/old-and-busted-tesasing-formery-fashionable-websites-from-Reddit-data/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/old-and-busted-tesasing-formery-fashionable-websites-from-Reddit-data/</guid>
      <description>&lt;p&gt;Anyone who spends even a little bit of time on the Internet knows how fickle and volatile the cultural scene is. And
there&amp;rsquo;s perhaps no greater exemplar of this volatility than Reddit. For good or bad, Reddit communities often serve as
tastemakers for the Internet at large. If your content is visible on Reddit, chances are that things are going great for
you. And if not, well, maybe not&amp;hellip;&lt;/p&gt;

&lt;p&gt;The topic for today&amp;rsquo;s post is pretty simple&amp;mdash;I&amp;rsquo;m just going to show off a cool analysis related to this
observation. The data set will be a &lt;a href=&#34;https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/&#34;&gt;JSON dump of all Reddit
submissions&lt;/a&gt; from
2006 up through Summer 2015, which I&amp;rsquo;ll be analyzing for site URLs that &lt;em&gt;used&lt;/em&gt; to be popular but have recently fallen
out of favor. The size of this dump was fairly formidable (about a quarter-terabyte uncompressed), so on the backend
this analysis was facilitated by running Drill in a cluster configuration on a Hadoop filesystem. This kept the runtime
of the somewhat sophisticated query I needed to perform down to well under an hour.&lt;/p&gt;

&lt;p&gt;So how exactly does one go about determining which Reddit submission URLs are, technically speaking, &amp;ldquo;old and busted&amp;rdquo;?
Well, I started off my analysis by constructing a view to keep track of the relevant parameters and convert them to the
correct format and units. In my case I&amp;rsquo;m interested the number of times a URL shows up in a submission, the average
posting date for each URL, and the standard deviation in submission dates (in units of days):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW reddit AS SELECT domain, COUNT(domain) counts, TO_TIMESTAMP(avg(CAST(created_utc AS FLOAT))) avg_date, STDDEV(CAST(created_utc AS FLOAT))/86400 std_dev_days
       FROM hdfs.`/data/RS_full_corpus.json`
   GROUP BY domain;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now it becomes fairly easy to construct a query on top of this view that looks for websites with old average
submission dates that also exhibit fairly strongly clustering (which I&amp;rsquo;ll define as an average submission date older
than Jan. 1, 2011 with a standard deviation in submission dates of less than 600 days). These two in combination will
define our &amp;ldquo;old and busted&amp;rdquo; criterion.&lt;/p&gt;

&lt;p&gt;The query looks like this, and returns these results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT * FROM reddit WHERE std_dev_days &amp;lt; 600 AND avg_date &amp;lt; &#39;2011-01-01 00:00:00&#39; ORDER BY counts DESC LIMIT 20;
+------------------------+---------+--------------------------+---------------------+
|         domain         | counts  |         avg_date         |    std_dev_days     |
+------------------------+---------+--------------------------+---------------------+
| self.reddit.com        | 647870  | 2010-09-02 13:56:28.448  | 301.71035168793304  |
| examiner.com           | 151810  | 2010-12-16 23:26:30.526  | 585.7963478654447   |
| news.bbc.co.uk         | 88227   | 2009-10-19 22:09:54.047  | 499.8828691832385   |
| squidoo.com            | 68613   | 2010-02-15 05:45:45.863  | 457.7467960677672   |
| hubpages.com           | 57290   | 2010-01-30 02:18:28.314  | 353.1463760830986   |
| msnbc.msn.com          | 43643   | 2010-06-13 13:34:33.274  | 491.4442681051865   |
| associatedcontent.com  | 24146   | 2009-08-18 17:40:57.408  | 403.1500022424656   |
| tinyurl.com            | 24070   | 2010-08-28 15:11:38.083  | 471.1292710623187   |
| self.programming       | 20689   | 2009-11-08 14:04:07.185  | 239.53706245104482  |
| physorg.com            | 17431   | 2010-09-27 02:00:58.908  | 435.832257804899    |
| ehow.com               | 16228   | 2009-09-19 05:58:19.334  | 524.9960557567199   |
| gather.com             | 15387   | 2010-06-28 12:05:28.479  | 365.76299060306405  |
| english.aljazeera.net  | 14573   | 2010-10-02 14:05:55.813  | 360.5432415872375   |
| subimg.net             | 13459   | 2010-09-29 09:48:42.801  | 278.7920245425415   |
| helium.com             | 13337   | 2009-09-28 16:38:56.651  | 441.641404727207    |
| sports.espn.go.com     | 12416   | 2010-10-17 19:30:23.67   | 493.84151020454976  |
| rapidsharelist.net     | 12128   | 2010-04-26 15:53:44.063  | 14.947036935681274  |
| waronyou.com           | 12036   | 2009-04-13 17:51:18.434  | 184.16688348655214  |
| timesonline.co.uk      | 11100   | 2009-05-09 01:23:30.756  | 288.42826940864705  |
| open.salon.com         | 10727   | 2010-08-10 09:48:09.563  | 494.63567877466306  |
+------------------------+---------+--------------------------+---------------------+
20 rows selected (2490.628 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using this list as a starting point, I started to do a little digging (see Table 1) into why these sites are no longer
popular. What I found is that there a lot of reasons a site may end up in this ignominious category&amp;mdash;anything from
a shift in the Google search algorithm to a simple redirect to a new URL. Other explanations included acquisition, a
collapsed business model, and an outright ban of the URL from Reddit by site administrators.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p style=&#34;text-align: center;&#34;&gt;
&lt;img style=&#34;max-width: 80%;&#34; src=&#34;kstirman.github.io/bookshelf/img/reddit_url_table.png&#34;&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: center; font-style: italic;&#34;&gt;&lt;b&gt;Table 1&lt;/b&gt;: A selection of formerly-fashionable websites
submitted to Reddit, and the suspected reason for their fall from grace.&lt;/p&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;This is pretty interesting stuff, right? I&amp;rsquo;m definitely looking forward to using Drill to hunt for even more trends in
the Reddit submission data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Setting up an SQL interface to a sharded MongoDB</title>
      <link>/kstirman.github.io/bookshelf/blog/setting-up-an-SQL-interface-to-a-sharded-MongoDB/</link>
      <pubDate>Wed, 02 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/setting-up-an-SQL-interface-to-a-sharded-MongoDB/</guid>
      <description>&lt;p&gt;Salutations, data fans! Today I&amp;rsquo;d like to demonstrate how to use Apache Drill to query a sharded MongoDB database
running on a computer cluster.&lt;/p&gt;

&lt;p&gt;To begin with, I&amp;rsquo;m going to assume you already know how to do a sharded Mongo setup (the &lt;a href=&#34;https://docs.mongodb.org/manual/sharding/&#34;&gt;official
documentation&lt;/a&gt; is very helpful), or you already have access to a system with
a sharded MongoDB in place. With this out of the way, I can focus on the Drill setup.&lt;/p&gt;

&lt;p&gt;When running in a multiple machine environment, Drill can increase its efficiency for very large queries by coordinating
between several instances of a service called a &amp;lsquo;drillbit.&amp;rsquo; To help track these instances we use a piece of software
called &lt;a href=&#34;https://zookeeper.apache.org/&#34;&gt;Apache Zookeeper&lt;/a&gt;. Once Zookeeper is installed, you should make a &amp;lsquo;zoo.cfg&amp;rsquo; file
in its &amp;lsquo;./conf&amp;rsquo; directory. This simple one works fine for our purposes (create a &amp;lsquo;dataDir&amp;rsquo; as needed):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tickTime=2000
dataDir=/home/cluster-user/zookeeper/data
clientPort=2183
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you should download and install Drill to each of the nodes in the cluster that you&amp;rsquo;d like a drillbit to run on. For
my setup I&amp;rsquo;ve chosen to run a drillbit on each node that contains a MongoDB shard. This is a good idea because Drill
considers the location of data stores when it performs queries. The ./conf/drill-override.conf file for each install
should be edited to reflect the IP address and port of your Zookeeper. For instance, you could write:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drill.exec: {
  cluster-id: &amp;quot;drill-cluster-example&amp;quot;,
  zk.connect: &amp;quot;172.31.0.0:2183&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where the name &amp;ldquo;drill-cluster-example&amp;rdquo; is the same for each node.&lt;/p&gt;

&lt;p&gt;With this minor bit of configuration out of the way, all that&amp;rsquo;s left is to start the software. First bring up the
Zookeeper server with &amp;lsquo;~/zookeeper/bin/zkServer.sh start&amp;rsquo;, and then launch the drillbits by running
&amp;lsquo;~/apache-drill/bin/drillbit.sh start&amp;rsquo; on each node.
After you&amp;rsquo;ve started everything, you can run &amp;lsquo;drill-conf&amp;rsquo; to bring you to an SQL prompt. If all is well, you
should be able to run &lt;code&gt;SELECT * FROM sys.drillbits;&lt;/code&gt; and see each drillbit on the cluster report in. For example on a
three machine test cluster I saw the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; select * from sys.drillbits;
+----------------------------------------------+------------+---------------+------------+----------+
|                   hostname                   | user_port  | control_port  | data_port  | current  |
+----------------------------------------------+------------+---------------+------------+----------+
| ip-172-31-41-122.us-west-2.compute.internal  | 31010      | 31011         | 31012      | false    |
| ip-172-31-41-120.us-west-2.compute.internal  | 31010      | 31011         | 31012      | false    |
| ip-172-31-41-121.us-west-2.compute.internal  | 31010      | 31011         | 31012      | true     |
+----------------------------------------------+------------+---------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now all that&amp;rsquo;s left is to activate the Drill interface to MongoDB. Open the Drill Web Console by visiting one of the
nodes that&amp;rsquo;s running a drillbit (use port 8047, just like before). From here click &amp;ldquo;Enable&amp;rdquo; next to the &amp;ldquo;mongo&amp;rdquo; plugin,
and then hit &amp;ldquo;Update.&amp;rdquo; Now change &amp;ldquo;localhost:27017&amp;rdquo; in the &amp;ldquo;connection&amp;rdquo; field to correspond to the IP and port of the
machine that runs your &amp;lsquo;mongos&amp;rsquo; process. If you have more than one &amp;ldquo;mongos&amp;rdquo; you can specify them all like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
  &amp;quot;connection&amp;quot;: &amp;quot;mongodb://172.31.41.120:27017,172.31.41.121:27017,172.31.41.122:27017&amp;quot;,
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it! You&amp;rsquo;re now all set to run SQL queries on your sharded MongoDB via Drill. For an example of how to
interface with some complicated JSON-based data that&amp;rsquo;s typical of MongoDB collections, check out &lt;a href=&#34;http://www.dremio.com/blog/bless-this-mess-working-with-complicated-json-structure-in-sql/&#34;&gt;this
other&lt;/a&gt; post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrapping an interactive SQL environment on Amazon EMR</title>
      <link>/kstirman.github.io/bookshelf/blog/bootstrapping-an-interactive-SQL-environment-on-Amazon-EMR/</link>
      <pubDate>Tue, 17 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/bootstrapping-an-interactive-SQL-environment-on-Amazon-EMR/</guid>
      <description>&lt;p&gt;Amazon&amp;rsquo;s Elastic MapReduce (EMR) service is a great way to create on-demand Hadoop clusters for ad hoc data analysis.
The web based interface for the service is simple to use, and at its most basic level creating an EMR cluster is as easy
as specifying the number and type of machines to use. But you can also instruct each computer to execute something
called a &amp;lsquo;bootstrap action&amp;rsquo; just before coming online. The contents of this script can direct the cluster to
automatically prepare itself for data crunching operations that have dependencies which lie outside of the primary
Hadoop toolset. In this post I&amp;rsquo;ll show you how to use an EMR bootstrap action provided by Dremio to install Drill on
each node in the cluster. As I&amp;rsquo;ve discussed before, Drill is great for a lot of stuff, and it&amp;rsquo;s a fast and easy way to
get near-realtime access to your data via a SQL interface.&lt;/p&gt;

&lt;p&gt;Launching a new EMR cluster that comes with Drill pre-installed is almost identical to launching one without it. The
only extra step is that you need to specify the location on Amazon S3 of a bootstrap action script. You can do this
under the &amp;ldquo;Bootstrap Actions&amp;rdquo; heading of the EMR &amp;ldquo;Advanced Options&amp;rdquo; page for cluster creation (it&amp;rsquo;s the second settings
grouping from the bottom of the page). Just select &amp;ldquo;Custom action&amp;rdquo; from the dropdown list, press &amp;ldquo;Configure and add&amp;rdquo;,
and then specify Dremio&amp;rsquo;s script for installing Drill (hosted at s3://repo.dremio.com/aws/emr/bootstrapDrill.py) in the
&amp;ldquo;JAR location&amp;rdquo; field.&lt;/p&gt;

&lt;p&gt;Once your cluster is up and ready, you should be able to log in and run &lt;code&gt;drill-conf&lt;/code&gt; to start Drill&amp;rsquo;s SQL prompt. Type
&lt;code&gt;SELECT * FROM sys.drillbits;&lt;/code&gt; and you&amp;rsquo;ll see all of the available drillbit services on your cluster (one for each
machine) listed like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT * FROM sys.drillbits;
+---------------------------------------------+------------+---------------+------------+----------+
|                  hostname                   | user_port  | control_port  | data_port  | current  |
+---------------------------------------------+------------+---------------+------------+----------+
| ip-172-31-23-93.us-west-2.compute.internal  | 31010      | 31011         | 31012      | true     |
| ip-172-31-26-64.us-west-2.compute.internal  | 31010      | 31011         | 31012      | false    |
| ip-172-31-23-94.us-west-2.compute.internal  | 31010      | 31011         | 31012      | false    |
+---------------------------------------------+------------+---------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since this script comes with the HDFS plugin for Drill pre-installed, you can start querying Hadoop data right away with
commands like &lt;code&gt;SELECT * FROM hdfs.`/data/userdata.json`&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can also enable access to files stored on Amazon S3 by specifying the name of an S3 bucket (without a leading
&amp;ldquo;s3://&amp;ldquo;) in the &amp;ldquo;Optional arguments&amp;rdquo; box of the &amp;ldquo;Add Bootstrap Action&amp;rdquo; dialog of the EMR web GUI. The name for this
plugin will be &amp;ldquo;s3,&amp;rdquo; so you can query a file in the bucket you specified with just &lt;code&gt;SELECT * FROM s3.`myfile.csv`&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re curious about using Drill in a cluster, pairing Amazon&amp;rsquo;s EMR service with this bootstrap action could be a
great place to start.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>