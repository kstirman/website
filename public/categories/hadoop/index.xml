<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dremio | reimagining data analytics for the modern world</title>
    <link>kstirman.github.io/bookshelf/categories/hadoop/index.xml</link>
    <description>Recent content on dremio | reimagining data analytics for the modern world</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="kstirman.github.io/bookshelf/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Securing SQL on Hadoop, Part 2: Installing and configuring Drill</title>
      <link>/kstirman.github.io/bookshelf/blog/securing-SQL-on-Hadoop-part-2-installing-and-configuring-Drill/</link>
      <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/securing-SQL-on-Hadoop-part-2-installing-and-configuring-Drill/</guid>
      <description>

&lt;p&gt;Today we&amp;rsquo;re going to pick up where we left off in &lt;a href=&#34;http://www.dremio.com/blog/securing-sql-on-hadoop-part-1-installing-cdh-and-kerberos/&#34;&gt;Part
1&lt;/a&gt; of my two-parter about setting
up a CDH cluster to perform secure SQL queries on an HDFS store. As you recall, last time we had just finished using
Cloudera Manager&amp;rsquo;s wizard to finalize a Kerberos configuration, and all of the cluster services had come back online
using the new security system so our basic HDFS cluster set-up was good to go. All that&amp;rsquo;s left to do now is install and
configure the piece of software that implements the SQL interface: Apache Drill.&lt;/p&gt;

&lt;h2 id=&#34;step-1-drill-prerequisites&#34;&gt;Step 1.) Drill prerequisites&lt;/h2&gt;

&lt;p&gt;First things first: We&amp;rsquo;re gonna need Java 7 or better on our CDH machines. Following some useful information found
&lt;a href=&#34;http://lifeonubuntu.com/ubuntu-missing-add-apt-repository-command/&#34;&gt;here&lt;/a&gt; and
&lt;a href=&#34;http://askubuntu.com/questions/508546/howto-upgrade-java-on-ubuntu-14-04-lts&#34;&gt;here&lt;/a&gt;, we can set up a new package
repository and pull it down using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install software-properties-common python-software-properties
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java7-installer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Remember to do this for each node on your cluster!)&lt;/p&gt;

&lt;h2 id=&#34;step-2-install-drill&#34;&gt;Step 2.) Install Drill&lt;/h2&gt;

&lt;p&gt;Time to install Drill and tell it where the cluster&amp;rsquo;s &amp;lsquo;Zookeeper&amp;rsquo; is (REMEMBER: Just like installing Java, this step
needs to be done on every node). Start by downloading and unpacking the software:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget http://apache.osuosl.org/drill/drill-1.4.0/apache-drill-1.4.0.tar.gz
$ tar xzvf apache-drill-1.4.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next edit Drill&amp;rsquo;s &lt;code&gt;conf/drill-override.conf&lt;/code&gt; file so that it looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drill.exec: {
  cluster-id: &amp;quot;drill-cdh-cluster&amp;quot;,
  zk.connect: &amp;quot;&amp;lt;ZOOKEEPER IP&amp;gt;:2181&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &amp;lsquo;ZOOKEEPER IP&amp;rsquo; should be changed to the IP of the CDH machine that runs the zookeeper process (this is probably
the same as the Main Node from the last article).&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like, you can add Drill to the system path by adding this to &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PATH=$PATH:/apache-drill-1.4.0/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-3-configuring-drill-for-hdfs&#34;&gt;Step 3.) Configuring Drill for HDFS&lt;/h2&gt;

&lt;p&gt;Now we need to set up Drill so that it can read our cluster&amp;rsquo;s HDFS. Open the Drill Web Console by going to
http://&amp;lt;MAIN NODE IP&amp;gt;:8047. Click &amp;lsquo;Storage&amp;rsquo; at the top and then &amp;lsquo;Update&amp;rsquo; next to the dfs plugin and copy the JSON
that you find in the text field. Next make a new storage plugin called &amp;lsquo;hdfs&amp;rsquo; (previous page) and then paste in the text
you just copied, replacing the &amp;lsquo;null&amp;rsquo; that&amp;rsquo;s already there.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll proceed by making a small modification that turns this standard &amp;lsquo;dfs&amp;rsquo; plugin in into our new one for HDFS.&lt;/p&gt;

&lt;p&gt;Take the line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;connection&amp;quot;: &amp;quot;file:///&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and replace it with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;connection&amp;quot;: &amp;quot;hdfs://&amp;lt;ADDRESS OF HDFS NAMENODE&amp;gt;:8020&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hit &amp;ldquo;Create&amp;rdquo; and now Drill is ready to read your HDFS data!&lt;/p&gt;

&lt;h2 id=&#34;step-4-enabling-kerberos-support-for-drill&#34;&gt;Step 4.) Enabling Kerberos support for Drill&lt;/h2&gt;

&lt;p&gt;So now we have HDFS, Kerberos, &lt;em&gt;and&lt;/em&gt; Drill, but currently Drill can&amp;rsquo;t talk to the HDFS we have running because it
requires authentication. Let&amp;rsquo;s fix that.&lt;/p&gt;

&lt;p&gt;First we should make an HDFS superuser account as indicated in this &lt;a href=&#34;http://www.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s5_hdfs_principal.html&#34;&gt;Cloudera
doc&lt;/a&gt;. On the Main Node, run
&lt;code&gt;sudo kadmin.local&lt;/code&gt; and add an &amp;lsquo;hdfs&amp;rsquo; principal with this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;addprinc hdfs@KERBEROS.CDH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Hit Ctrl-d to exit the prompt). In order to enable authentication with Kerberos, we also need to copy the file
&lt;code&gt;hadoop-yarn-api.jar&lt;/code&gt; into Drill&amp;rsquo;s class path:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp /opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/lib/hadoop/client/hadoop-yarn-api.jar ~/apache-drill/jars/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(NOTE: &lt;em&gt;The above step and the three following must be performed on each node of the cluster&lt;/em&gt;.)&lt;/p&gt;

&lt;p&gt;Next, Drill&amp;rsquo;s &lt;code&gt;conf/core-site.xml&lt;/code&gt; file should be edited to contain the following snippet of xml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.security.authentication&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that&amp;rsquo;s left to do is create an &amp;lsquo;hdfs&amp;rsquo; Kerberos ticket for the user that we&amp;rsquo;re logged into&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kinit hdfs@KERBEROS.CDH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then start up each of the drillbits&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drillbit.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now Drill has both the configuration and the authority to use our kerberized HDFS store. Give it a shot by opening up
a Drill prompt (&lt;code&gt;drill-conf&lt;/code&gt;) and trying a query.&lt;/p&gt;

&lt;p&gt;For example, here&amp;rsquo;s a test query on my handy 250 GB of Reddit data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT title, TO_TIMESTAMP(CAST(created_utc AS INT)) created, score FROM hdfs.`/data/RS_full_corpus.json` WHERE over_18 = false AND score &amp;gt;= 100 LIMIT 10;
+------------------------------------------------------------------------------+------------------------+--------+
|                                    title                                     |        created         | score  |
+------------------------------------------------------------------------------+------------------------+--------+
| Early Retirement Guide - Phillip Greenspun                                   | 2006-01-30 19:21:17.0  | 223    |
| Programming Like A Mathematician I: Closures                                 | 2006-01-29 18:14:30.0  | 178    |
| More than 1000 wikipedia alterations by US Representative Staffers           | 2006-01-29 12:57:45.0  | 304    |
| Great Design: What is Design?                                                | 2006-01-26 20:54:30.0  | 143    |
| Use Python (not Java) to teach programming                                   | 2006-01-26 19:43:23.0  | 167    |
| The Demotivators!                                                            | 2006-01-26 18:02:51.0  | 168    |
| Just how much can you achieve with pure CSS? Some amazing demonstrations.    | 2006-02-26 18:07:57.0  | 329    |
| A summary of two lectures by Alan Kay                                        | 2006-02-24 04:56:49.0  | 110    |
| How Intel could buy Hollywood and profit by selling more DRM-less machines.  | 2006-02-20 20:48:18.0  | 108    |
| &amp;quot;zeitgeist&amp;quot; reddits, covering popular topics, eg, the infamous cartoons      | 2006-02-19 19:33:59.0  | 299    |
+------------------------------------------------------------------------------+------------------------+--------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that wraps up Part 2 of this how-to guide. May all your queries be fruitful and secure!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Acknoweldgements: Many thanks to William Witt over on the user@drill.apache.org mailing list for providing crucial
information about Kerberos-Drill configuration!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Securing SQL on Hadoop, Part 1: Installing CDH and Kerberos</title>
      <link>/kstirman.github.io/bookshelf/blog/securing-SQL-on-Hadoop-part-1-installing-CDH-and-Kerberos/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/securing-SQL-on-Hadoop-part-1-installing-CDH-and-Kerberos/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In the Dremio Blog we&amp;rsquo;ve talked about pairing Drill with HDFS before, but never with the emphasis on security that so
often comes hand-in-hand with enterprise applications. So today&amp;rsquo;s post marks the beginning of a two part series
explaining how to set up a cluster environment that enables &lt;em&gt;secure&lt;/em&gt; SQL queries to data stored on HDFS. For this
article&amp;rsquo;s test &amp;lsquo;hardware&amp;rsquo; we&amp;rsquo;ll use six instances provisioned from Amazon&amp;rsquo;s EC2 service, while the core software
components will be provided by Cloudera&amp;rsquo;s Hadoop system, CDH, paired with Ubuntu 14.04. The &amp;lsquo;secure&amp;rsquo; element of this
cluster environment with be enabled via Kerberos, which is an industry standard in the realm of authentication software.&lt;/p&gt;

&lt;h2 id=&#34;step-1-cluster-set-up&#34;&gt;Step 1.) Cluster Set Up&lt;/h2&gt;

&lt;p&gt;In general I&amp;rsquo;ll be going into a significant amount of detail for this setup, since Kerberos configuration can be, well,
&lt;em&gt;hard&lt;/em&gt;. I am, however, going to assume right now that you know your way around the AWS Management Console, and can
handle setting up a few instances and configuring them by yourself. Remember to select &amp;lsquo;Ubuntu 14.04&amp;rsquo; for the operating
system, and make a security group for the cluster that opens all the the TCP and UDP ports between machines in the
cluster, just to be sure they can communicate freely with eachother. You&amp;rsquo;ll also want to open all ICMP ports (for ping),
and TCP port 7180 for the Cloudera Manager software that we&amp;rsquo;ll be using shortly. Things like system specs and storage
are obviously somewhat up to you to decide on, but I went with fairly muscular &amp;lsquo;m4.xlarge&amp;rsquo; instances that had 250 GB of
storage each (more than big enough for my Reddit submission corpus test data, which was featured in &lt;a href=&#34;http://www.dremio.com/blog/old-and-busted-teasing-formerly-fashionable-websites-from-reddit-data/&#34;&gt;this previous
article&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;step-2-install-cdh-with-cloudera-manager&#34;&gt;Step 2.) Install CDH with Cloudera Manager&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll perform the install of CDH (and thus, our HDFS store) by using Cloudera&amp;rsquo;s awesome Cloudera Manager software, which
is &lt;a href=&#34;http://www.cloudera.com/downloads/manager/5-5-1.html&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After you download the .bin file from this site to one of the nodes on your cluster, mark it as executable and run it
(from here on out I&amp;rsquo;ll call the node that you selected for this step the &amp;lsquo;Main Node&amp;rsquo;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ chmod +x cloudera-manager-installer.bin
$ sudo ./cloudera-manager-installer.bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once this software finishes running you can point a browser to the Main Node&amp;rsquo;s URL (available from the AWS EC2 web
interface) on port 7180. From here you can complete the install of CDH to rest of your cluster (the initial username and
password are both &amp;lsquo;admin&amp;rsquo;). There are various options that show up as you progress through the steps, but for this
simple example it&amp;rsquo;s sufficient to accept the defaults for most cases. Just be sure to check the &amp;ldquo;Install Oracle Java SE
Development Kit&amp;rdquo; and &amp;ldquo;Install Java Unlimited Strength Encryption Policy Files&amp;rdquo; boxes when they show up (this last one
can be relevant to Kerberos encryption). You&amp;rsquo;ll also need to specify the user &amp;lsquo;ubuntu&amp;rsquo; and the relevant private key
&amp;lsquo;.pem&amp;rsquo; file for your cluster when you hit the &amp;ldquo;Provide SSH login credentials&amp;rdquo; screen. And the basic &amp;ldquo;Core Hadoop&amp;rdquo; option
is fine when t comes to choosing what package set to install.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s really all there is to getting the Hadoop side of things running for this project. Shockingly easy, right?&lt;/p&gt;

&lt;p&gt;But now it&amp;rsquo;s time for Kerberos. Sigh.&lt;/p&gt;

&lt;h2 id=&#34;step-3a-install-and-configure-kerberos-main-node&#34;&gt;Step 3a.) Install and Configure Kerberos: Main Node&lt;/h2&gt;

&lt;p&gt;Okay, it&amp;rsquo;s not actually all that bad. All we have to do is install some packages on the cluster and do some small edits
to conf files before we can hand it off to the Clouder Manager wizard for Kerberos configuration (let me say it again:
Cloudera Manager is awesome).&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s begin with that needs to be done on the Main Node, which in the parlance of Kerberos will become the &amp;lsquo;KDC&amp;rsquo; (Key
Distribution Center) of our authentication system. As per this &lt;a href=&#34;https://help.ubuntu.com/community/Kerberos&#34;&gt;Ubuntu
documentation&lt;/a&gt; (which was a useful reference for many of the following
steps), go ahead and install these two packages&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install krb5-kdc krb5-admin-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;entering &amp;ldquo;KEBEROS.CDH&amp;rdquo; for the realm name in the first field that pops up, and specifying the Main Node&amp;rsquo;s internal DNS
name in the next two (again these can be found on the AWS EC2 instance page&amp;mdash;they&amp;rsquo;re formatted like
&amp;ldquo;ip-172.XXX.XXX.XXX.us-west-2.compute.internal&amp;rdquo;). Then run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo dpkg-reconfigure krb5-kdc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and select &amp;ldquo;Yes.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s time to edit the &lt;code&gt;/etc/krb5kdc/kdc.conf&lt;/code&gt; file as recommended in &lt;a href=&#34;http://www.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s4_kerb_wizard.html&#34;&gt;these Cloudera
instructions&lt;/a&gt;. Place the
lines&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;max_life = 1d
max_renewable_life = 7d
kdc_tcp_ports = 88
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;in the KERBEROS.CDH &amp;lsquo;[realms]&amp;rsquo; entry, replacing any similar lines and deleting &amp;lsquo;kdc_ports&amp;rsquo; both there underneath
&amp;lsquo;[kdcdefaults].&amp;rsquo; Alright! On to the next file.&lt;/p&gt;

&lt;p&gt;Create (or open) &lt;code&gt;/etc/krb5kdc/kadm5.acl&lt;/code&gt;, and insert this line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*/admin@KERBEROS.CDH    *
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it&amp;rsquo;s time to initialize the Kerberos realm with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo krb5_newrealm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cool. Now, we need to create the credentials that the Cloudera Manager wizard will use when it completes our Kerberos
setup. To do this enter the command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo kadmin.local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and from within the prompt that&amp;rsquo;s presented, type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;addprinc -pw &amp;lt;PASSWORD&amp;gt; cloudera-scm/admin@KERBEROS.CDH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now hit Crtl-D to exit the prompt, and install one last package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install ldap-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-3b-install-and-configure-kerberos-other-nodes&#34;&gt;Step 3b.) Install and Configure Kerberos: Other Nodes&lt;/h2&gt;

&lt;p&gt;Compared to the Main Node setup, this step is mercifully brief. For each other note in the cluster you just need to
install this package:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install krb5-user
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you&amp;rsquo;re presented with the text fields this time you can just hit enter, because the wizard will end up handling
this stuff for you in the end.&lt;/p&gt;

&lt;h2 id=&#34;step-4-use-the-cloudera-manager-kerberos-wizard&#34;&gt;Step 4.) Use the Cloudera Manager Kerberos wizard&lt;/h2&gt;

&lt;p&gt;To start the Kerberos wizard, select &amp;ldquo;Enable Kerberos&amp;rdquo; from the dropdown menu for the cluster (see Figure 1).&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p style=&#34;text-align: center;&#34;&gt;
&lt;img style=&#34;max-width: 100%;&#34; src=&#34;kstirman.github.io/bookshelf/img/cloudera_kerberos.jpg&#34;&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: center; font-style: italic;&#34;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Starting Cloudera Manager&amp;rsquo;s Kerberos wizard.&lt;/p&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;First you&amp;rsquo;re asked to check a bunch of boxes which are friendly reminders of all the prep work you need to do before the
wizard can take over. We&amp;rsquo;ve done all of this in the previous steps, so you can check away with wild abandon.&lt;/p&gt;

&lt;p&gt;On the next page put the AWS internal address for the Main Node in the field marked &amp;ldquo;KDC Server Host&amp;rdquo; and then
&amp;ldquo;KERBEROS.CDH&amp;rdquo; for the realm name. Hit &amp;ldquo;Continue&amp;rdquo; and then check the &amp;ldquo;Manage krb5.conf through Cloudera Manager&amp;rdquo; box.
The defaults here are fine.&lt;/p&gt;

&lt;p&gt;Now you need to enter the credentials that were set up in Step 3a. As per &lt;a href=&#34;http://www.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s3_cm_principal.html&#34;&gt;this Cloudera
doc&lt;/a&gt; we made the account
&amp;lsquo;cloudera-scm/admin&amp;rsquo;, so go ahead and put that username and whatever password you chose into this page.&lt;/p&gt;

&lt;p&gt;From here you can just hit &amp;ldquo;Continue&amp;rdquo; until you&amp;rsquo;re presented with a &amp;ldquo;Yes, I am ready to restart the cluster now&amp;rdquo;
checkbox. Check it, and (you guessed it!) hit &amp;ldquo;Continue&amp;rdquo; again. The wizard will now stop all the services and bring them
back up with Kerberos authentication activated.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s it! You now have a Kerberos-secured CDH cluster!&lt;/p&gt;

&lt;p&gt;This is cool in itself, but in my next post we&amp;rsquo;ll get to the real heart of the matter: Installing and configuring Apache
Drill on this system so that we can perform the secure SQL queries that I advertized in the article title. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Old and busted: Teasing formerly-fashionable websites from Reddit data</title>
      <link>/kstirman.github.io/bookshelf/blog/old-and-busted-tesasing-formery-fashionable-websites-from-Reddit-data/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/old-and-busted-tesasing-formery-fashionable-websites-from-Reddit-data/</guid>
      <description>&lt;p&gt;Anyone who spends even a little bit of time on the Internet knows how fickle and volatile the cultural scene is. And
there&amp;rsquo;s perhaps no greater exemplar of this volatility than Reddit. For good or bad, Reddit communities often serve as
tastemakers for the Internet at large. If your content is visible on Reddit, chances are that things are going great for
you. And if not, well, maybe not&amp;hellip;&lt;/p&gt;

&lt;p&gt;The topic for today&amp;rsquo;s post is pretty simple&amp;mdash;I&amp;rsquo;m just going to show off a cool analysis related to this
observation. The data set will be a &lt;a href=&#34;https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/&#34;&gt;JSON dump of all Reddit
submissions&lt;/a&gt; from
2006 up through Summer 2015, which I&amp;rsquo;ll be analyzing for site URLs that &lt;em&gt;used&lt;/em&gt; to be popular but have recently fallen
out of favor. The size of this dump was fairly formidable (about a quarter-terabyte uncompressed), so on the backend
this analysis was facilitated by running Drill in a cluster configuration on a Hadoop filesystem. This kept the runtime
of the somewhat sophisticated query I needed to perform down to well under an hour.&lt;/p&gt;

&lt;p&gt;So how exactly does one go about determining which Reddit submission URLs are, technically speaking, &amp;ldquo;old and busted&amp;rdquo;?
Well, I started off my analysis by constructing a view to keep track of the relevant parameters and convert them to the
correct format and units. In my case I&amp;rsquo;m interested the number of times a URL shows up in a submission, the average
posting date for each URL, and the standard deviation in submission dates (in units of days):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW reddit AS SELECT domain, COUNT(domain) counts, TO_TIMESTAMP(avg(CAST(created_utc AS FLOAT))) avg_date, STDDEV(CAST(created_utc AS FLOAT))/86400 std_dev_days
       FROM hdfs.`/data/RS_full_corpus.json`
   GROUP BY domain;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now it becomes fairly easy to construct a query on top of this view that looks for websites with old average
submission dates that also exhibit fairly strongly clustering (which I&amp;rsquo;ll define as an average submission date older
than Jan. 1, 2011 with a standard deviation in submission dates of less than 600 days). These two in combination will
define our &amp;ldquo;old and busted&amp;rdquo; criterion.&lt;/p&gt;

&lt;p&gt;The query looks like this, and returns these results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT * FROM reddit WHERE std_dev_days &amp;lt; 600 AND avg_date &amp;lt; &#39;2011-01-01 00:00:00&#39; ORDER BY counts DESC LIMIT 20;
+------------------------+---------+--------------------------+---------------------+
|         domain         | counts  |         avg_date         |    std_dev_days     |
+------------------------+---------+--------------------------+---------------------+
| self.reddit.com        | 647870  | 2010-09-02 13:56:28.448  | 301.71035168793304  |
| examiner.com           | 151810  | 2010-12-16 23:26:30.526  | 585.7963478654447   |
| news.bbc.co.uk         | 88227   | 2009-10-19 22:09:54.047  | 499.8828691832385   |
| squidoo.com            | 68613   | 2010-02-15 05:45:45.863  | 457.7467960677672   |
| hubpages.com           | 57290   | 2010-01-30 02:18:28.314  | 353.1463760830986   |
| msnbc.msn.com          | 43643   | 2010-06-13 13:34:33.274  | 491.4442681051865   |
| associatedcontent.com  | 24146   | 2009-08-18 17:40:57.408  | 403.1500022424656   |
| tinyurl.com            | 24070   | 2010-08-28 15:11:38.083  | 471.1292710623187   |
| self.programming       | 20689   | 2009-11-08 14:04:07.185  | 239.53706245104482  |
| physorg.com            | 17431   | 2010-09-27 02:00:58.908  | 435.832257804899    |
| ehow.com               | 16228   | 2009-09-19 05:58:19.334  | 524.9960557567199   |
| gather.com             | 15387   | 2010-06-28 12:05:28.479  | 365.76299060306405  |
| english.aljazeera.net  | 14573   | 2010-10-02 14:05:55.813  | 360.5432415872375   |
| subimg.net             | 13459   | 2010-09-29 09:48:42.801  | 278.7920245425415   |
| helium.com             | 13337   | 2009-09-28 16:38:56.651  | 441.641404727207    |
| sports.espn.go.com     | 12416   | 2010-10-17 19:30:23.67   | 493.84151020454976  |
| rapidsharelist.net     | 12128   | 2010-04-26 15:53:44.063  | 14.947036935681274  |
| waronyou.com           | 12036   | 2009-04-13 17:51:18.434  | 184.16688348655214  |
| timesonline.co.uk      | 11100   | 2009-05-09 01:23:30.756  | 288.42826940864705  |
| open.salon.com         | 10727   | 2010-08-10 09:48:09.563  | 494.63567877466306  |
+------------------------+---------+--------------------------+---------------------+
20 rows selected (2490.628 seconds)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using this list as a starting point, I started to do a little digging (see Table 1) into why these sites are no longer
popular. What I found is that there a lot of reasons a site may end up in this ignominious category&amp;mdash;anything from
a shift in the Google search algorithm to a simple redirect to a new URL. Other explanations included acquisition, a
collapsed business model, and an outright ban of the URL from Reddit by site administrators.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p style=&#34;text-align: center;&#34;&gt;
&lt;img style=&#34;max-width: 80%;&#34; src=&#34;kstirman.github.io/bookshelf/img/reddit_url_table.png&#34;&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: center; font-style: italic;&#34;&gt;&lt;b&gt;Table 1&lt;/b&gt;: A selection of formerly-fashionable websites
submitted to Reddit, and the suspected reason for their fall from grace.&lt;/p&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;This is pretty interesting stuff, right? I&amp;rsquo;m definitely looking forward to using Drill to hunt for even more trends in
the Reddit submission data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrapping an interactive SQL environment on Amazon EMR</title>
      <link>/kstirman.github.io/bookshelf/blog/bootstrapping-an-interactive-SQL-environment-on-Amazon-EMR/</link>
      <pubDate>Tue, 17 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/kstirman.github.io/bookshelf/blog/bootstrapping-an-interactive-SQL-environment-on-Amazon-EMR/</guid>
      <description>&lt;p&gt;Amazon&amp;rsquo;s Elastic MapReduce (EMR) service is a great way to create on-demand Hadoop clusters for ad hoc data analysis.
The web based interface for the service is simple to use, and at its most basic level creating an EMR cluster is as easy
as specifying the number and type of machines to use. But you can also instruct each computer to execute something
called a &amp;lsquo;bootstrap action&amp;rsquo; just before coming online. The contents of this script can direct the cluster to
automatically prepare itself for data crunching operations that have dependencies which lie outside of the primary
Hadoop toolset. In this post I&amp;rsquo;ll show you how to use an EMR bootstrap action provided by Dremio to install Drill on
each node in the cluster. As I&amp;rsquo;ve discussed before, Drill is great for a lot of stuff, and it&amp;rsquo;s a fast and easy way to
get near-realtime access to your data via a SQL interface.&lt;/p&gt;

&lt;p&gt;Launching a new EMR cluster that comes with Drill pre-installed is almost identical to launching one without it. The
only extra step is that you need to specify the location on Amazon S3 of a bootstrap action script. You can do this
under the &amp;ldquo;Bootstrap Actions&amp;rdquo; heading of the EMR &amp;ldquo;Advanced Options&amp;rdquo; page for cluster creation (it&amp;rsquo;s the second settings
grouping from the bottom of the page). Just select &amp;ldquo;Custom action&amp;rdquo; from the dropdown list, press &amp;ldquo;Configure and add&amp;rdquo;,
and then specify Dremio&amp;rsquo;s script for installing Drill (hosted at s3://repo.dremio.com/aws/emr/bootstrapDrill.py) in the
&amp;ldquo;JAR location&amp;rdquo; field.&lt;/p&gt;

&lt;p&gt;Once your cluster is up and ready, you should be able to log in and run &lt;code&gt;drill-conf&lt;/code&gt; to start Drill&amp;rsquo;s SQL prompt. Type
&lt;code&gt;SELECT * FROM sys.drillbits;&lt;/code&gt; and you&amp;rsquo;ll see all of the available drillbit services on your cluster (one for each
machine) listed like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT * FROM sys.drillbits;
+---------------------------------------------+------------+---------------+------------+----------+
|                  hostname                   | user_port  | control_port  | data_port  | current  |
+---------------------------------------------+------------+---------------+------------+----------+
| ip-172-31-23-93.us-west-2.compute.internal  | 31010      | 31011         | 31012      | true     |
| ip-172-31-26-64.us-west-2.compute.internal  | 31010      | 31011         | 31012      | false    |
| ip-172-31-23-94.us-west-2.compute.internal  | 31010      | 31011         | 31012      | false    |
+---------------------------------------------+------------+---------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since this script comes with the HDFS plugin for Drill pre-installed, you can start querying Hadoop data right away with
commands like &lt;code&gt;SELECT * FROM hdfs.`/data/userdata.json`&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can also enable access to files stored on Amazon S3 by specifying the name of an S3 bucket (without a leading
&amp;ldquo;s3://&amp;ldquo;) in the &amp;ldquo;Optional arguments&amp;rdquo; box of the &amp;ldquo;Add Bootstrap Action&amp;rdquo; dialog of the EMR web GUI. The name for this
plugin will be &amp;ldquo;s3,&amp;rdquo; so you can query a file in the bucket you specified with just &lt;code&gt;SELECT * FROM s3.`myfile.csv`&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re curious about using Drill in a cluster, pairing Amazon&amp;rsquo;s EMR service with this bootstrap action could be a
great place to start.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>